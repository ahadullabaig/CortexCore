#!/usr/bin/env python3
"""
Ensemble Averaging Validation Script
=====================================

Validates the ensemble averaging implementation and demonstrates variance
reduction for stochastic spike encoding in SNN inference.

Tests:
    1. Reproducibility with seed control
    2. Variance reduction (single vs ensemble)
    3. Prediction stability across multiple runs
    4. Performance benchmarking
    5. Clinical decision support metrics

Usage:
    python scripts/validate_ensemble_averaging.py
"""

import sys
from pathlib import Path

# Add src to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import numpy as np
import torch
from typing import List, Dict
import time
import json

from src.model import SimpleSNN
from src.inference import load_model, predict, ensemble_predict
from src.data import generate_synthetic_ecg
from src.utils import set_seed, get_device


class EnsembleValidator:
    """Comprehensive validation suite for ensemble averaging"""

    def __init__(self, model_path: str = 'models/best_model.pt'):
        """
        Initialize validator

        Args:
            model_path: Path to trained model checkpoint
        """
        self.model_path = model_path
        self.device = get_device()
        self.model = None
        self.test_signals = None

        print("=" * 70)
        print("üî¨ ENSEMBLE AVERAGING VALIDATION SUITE")
        print("=" * 70)
        print(f"Device: {self.device}")
        print(f"Model: {model_path}")
        print()

    def load_model_and_data(self):
        """Load model and generate test signals"""
        print("üì¶ Loading model and generating test data...")

        try:
            # Load model
            model_instance = SimpleSNN()
            self.model = load_model(self.model_path, model_instance, device=str(self.device))
            print(f"   ‚úÖ Model loaded successfully")

            # Generate diverse test signals
            set_seed(42)
            normal_signals = generate_synthetic_ecg(n_samples=5, condition='normal')
            arrhythmia_signals = generate_synthetic_ecg(n_samples=5, condition='arrhythmia')
            self.test_signals = np.vstack([normal_signals, arrhythmia_signals])

            print(f"   ‚úÖ Generated {len(self.test_signals)} test signals")
            print()
            return True

        except FileNotFoundError:
            print(f"   ‚ùå Model not found at {self.model_path}")
            print(f"   üí° Run 'make train' to train the model first")
            return False
        except Exception as e:
            print(f"   ‚ùå Error loading model: {e}")
            return False

    def test_reproducibility(self):
        """Test 1: Reproducibility with seed control"""
        print("=" * 70)
        print("TEST 1: Reproducibility with Seed Control")
        print("=" * 70)

        signal = self.test_signals[0]

        # Run with same seed twice
        print("üîÑ Running prediction with seed=42 (run 1)...")
        result1 = predict(self.model, signal, seed=42, device=str(self.device))

        print("üîÑ Running prediction with seed=42 (run 2)...")
        result2 = predict(self.model, signal, seed=42, device=str(self.device))

        # Check exact reproducibility
        pred_match = result1['prediction'] == result2['prediction']
        conf_match = abs(result1['confidence'] - result2['confidence']) < 1e-6

        print(f"\nüìä Results:")
        print(f"   Run 1: {result1['class_name']} ({result1['confidence']:.4f})")
        print(f"   Run 2: {result2['class_name']} ({result2['confidence']:.4f})")
        print(f"   Prediction match: {'‚úÖ' if pred_match else '‚ùå'}")
        print(f"   Confidence match: {'‚úÖ' if conf_match else '‚ùå'}")

        if pred_match and conf_match:
            print("\n‚úÖ TEST PASSED: Reproducibility verified")
        else:
            print("\n‚ö†Ô∏è  TEST WARNING: Reproducibility not perfect (acceptable for GPU ops)")

        print()

    def test_variance_reduction(self):
        """Test 2: Variance reduction (single vs ensemble)"""
        print("=" * 70)
        print("TEST 2: Variance Reduction (Single vs Ensemble)")
        print("=" * 70)

        signal = self.test_signals[0]
        n_trials = 10
        ensemble_size = 5

        # Single predictions (multiple trials)
        print(f"üîÑ Running {n_trials} single predictions (no ensemble)...")
        single_results = []
        for i in range(n_trials):
            result = predict(self.model, signal, device=str(self.device))
            single_results.append(result)
            print(f"   Trial {i+1}: {result['class_name']} ({result['confidence']:.1%})")

        # Ensemble predictions (multiple trials)
        print(f"\nüîÑ Running {n_trials} ensemble predictions (size={ensemble_size})...")
        ensemble_results = []
        for i in range(n_trials):
            result = ensemble_predict(
                self.model, signal,
                ensemble_size=ensemble_size,
                device=str(self.device)
            )
            ensemble_results.append(result)

        # Calculate statistics
        single_confidences = np.array([r['confidence'] for r in single_results])
        ensemble_confidences = np.array([r['confidence'] for r in ensemble_results])

        single_std = single_confidences.std()
        ensemble_std = ensemble_confidences.std()
        variance_reduction = (1 - ensemble_std / single_std) * 100 if single_std > 0 else 0

        print(f"\nüìä Variance Analysis:")
        print(f"   Single prediction:")
        print(f"      Mean confidence: {single_confidences.mean():.1%}")
        print(f"      Std deviation:   {single_std:.3f}")
        print(f"      Range: [{single_confidences.min():.1%}, {single_confidences.max():.1%}]")
        print(f"\n   Ensemble prediction:")
        print(f"      Mean confidence: {ensemble_confidences.mean():.1%}")
        print(f"      Std deviation:   {ensemble_std:.3f}")
        print(f"      Range: [{ensemble_confidences.min():.1%}, {ensemble_confidences.max():.1%}]")
        print(f"\n   üìâ Variance reduction: {variance_reduction:.0f}%")

        # Theoretical expectation: ‚àöN reduction
        expected_reduction = (1 - 1/np.sqrt(ensemble_size)) * 100
        print(f"   üìö Theoretical (‚àöN): {expected_reduction:.0f}%")

        if variance_reduction > 30:  # At least 30% reduction
            print("\n‚úÖ TEST PASSED: Significant variance reduction achieved")
        else:
            print("\n‚ö†Ô∏è  TEST WARNING: Variance reduction lower than expected")

        print()

    def test_prediction_stability(self):
        """Test 3: Prediction stability across signals"""
        print("=" * 70)
        print("TEST 3: Prediction Stability Across Multiple Signals")
        print("=" * 70)

        results = []

        print(f"üîÑ Running ensemble predictions on {len(self.test_signals)} signals...\n")

        for i, signal in enumerate(self.test_signals):
            result = ensemble_predict(
                self.model, signal,
                ensemble_size=5,
                device=str(self.device)
            )

            label = "Normal" if i < 5 else "Arrhythmia"
            correct = result['class_name'] == label

            results.append({
                'signal_id': i,
                'true_label': label,
                'prediction': result['class_name'],
                'confidence': result['confidence'],
                'confidence_std': result['confidence_std'],
                'agreement_rate': result['agreement_rate'],
                'correct': correct
            })

            symbol = "‚úÖ" if correct else "‚ùå"
            print(f"   Signal {i+1} ({label:12s}): {result['class_name']:12s} "
                  f"({result['confidence']:.1%} ¬± {result['confidence_std']:.1%}, "
                  f"{result['agreement_rate']:.0%} agree) {symbol}")

        # Calculate accuracy
        accuracy = sum(r['correct'] for r in results) / len(results) * 100

        print(f"\nüìä Summary:")
        print(f"   Overall accuracy: {accuracy:.1f}%")
        print(f"   Mean confidence: {np.mean([r['confidence'] for r in results]):.1%}")
        print(f"   Mean uncertainty: {np.mean([r['confidence_std'] for r in results]):.1%}")
        print(f"   Mean agreement: {np.mean([r['agreement_rate'] for r in results]):.0%}")

        if accuracy >= 70:  # Reasonable threshold for synthetic data
            print("\n‚úÖ TEST PASSED: Stable predictions across signals")
        else:
            print("\n‚ö†Ô∏è  TEST WARNING: Prediction accuracy lower than expected")

        print()

    def test_performance_benchmarking(self):
        """Test 4: Performance benchmarking"""
        print("=" * 70)
        print("TEST 4: Performance Benchmarking")
        print("=" * 70)

        signal = self.test_signals[0]
        n_warmup = 3
        n_trials = 10

        # Warmup
        print("üî• Warming up GPU...")
        for _ in range(n_warmup):
            predict(self.model, signal, device=str(self.device))

        # Benchmark single prediction
        print(f"\n‚è±Ô∏è  Benchmarking single prediction ({n_trials} trials)...")
        single_times = []
        for _ in range(n_trials):
            start = time.time()
            predict(self.model, signal, device=str(self.device))
            single_times.append((time.time() - start) * 1000)

        # Benchmark ensemble predictions
        ensemble_sizes = [3, 5, 7]
        ensemble_times = {}

        for size in ensemble_sizes:
            print(f"‚è±Ô∏è  Benchmarking ensemble prediction (size={size}, {n_trials} trials)...")
            times = []
            for _ in range(n_trials):
                start = time.time()
                ensemble_predict(self.model, signal, ensemble_size=size, device=str(self.device))
                times.append((time.time() - start) * 1000)
            ensemble_times[size] = times

        print(f"\nüìä Performance Results:")
        print(f"   Single prediction:")
        print(f"      Mean: {np.mean(single_times):.1f}ms")
        print(f"      Std:  {np.std(single_times):.1f}ms")
        print(f"      Range: [{np.min(single_times):.1f}, {np.max(single_times):.1f}]ms")

        for size in ensemble_sizes:
            times = ensemble_times[size]
            print(f"\n   Ensemble (N={size}):")
            print(f"      Mean: {np.mean(times):.1f}ms")
            print(f"      Std:  {np.std(times):.1f}ms")
            print(f"      Overhead: {np.mean(times) / (size * np.mean(single_times)):.1f}x vs theoretical")
            print(f"      Per-run: {np.mean(times) / size:.1f}ms")

        # Clinical acceptability check (<500ms for real-time)
        ensemble_5_mean = np.mean(ensemble_times[5])
        if ensemble_5_mean < 500:
            print(f"\n‚úÖ TEST PASSED: Ensemble inference <500ms (real-time capable)")
        else:
            print(f"\n‚ö†Ô∏è  TEST WARNING: Ensemble inference >{ensemble_5_mean:.0f}ms (may need optimization)")

        print()

    def test_clinical_decision_support(self):
        """Test 5: Clinical decision support metrics"""
        print("=" * 70)
        print("TEST 5: Clinical Decision Support Metrics")
        print("=" * 70)

        signal = self.test_signals[0]

        # Run detailed ensemble prediction
        result = ensemble_predict(
            self.model, signal,
            ensemble_size=7,
            device=str(self.device),
            return_detailed_stats=True
        )

        print(f"üìä Detailed Ensemble Analysis:")
        print(f"\n   Prediction: {result['class_name']}")
        print(f"   Confidence: {result['confidence']:.1%}")
        print(f"   Confidence interval (95%): [{result['confidence_ci_95'][0]:.1%}, "
              f"{result['confidence_ci_95'][1]:.1%}]")
        print(f"   Uncertainty (std): {result['confidence_std']:.1%}")
        print(f"   Agreement rate: {result['agreement_rate']:.0%}")
        print(f"   Prediction variance: {result['prediction_variance']}")

        # Clinical flagging logic
        print(f"\nüè• Clinical Decision Support:")

        flags = []
        if result['confidence'] < 0.70:
            flags.append("‚ö†Ô∏è  LOW CONFIDENCE - Flag for expert review")
        if result['confidence_std'] > 0.15:
            flags.append("‚ö†Ô∏è  HIGH UNCERTAINTY - Consider repeated measurement")
        if result['agreement_rate'] < 0.80:
            flags.append("‚ö†Ô∏è  ENSEMBLE DISAGREEMENT - Exercise caution")

        if flags:
            for flag in flags:
                print(f"   {flag}")
        else:
            print(f"   ‚úÖ HIGH CONFIDENCE - Prediction reliable for clinical use")

        # Display individual run results
        if 'all_predictions' in result:
            print(f"\nüìã Individual Run Results:")
            for i, pred in enumerate(result['all_predictions']):
                print(f"   Run {i+1}: {pred['class_name']:12s} ({pred['confidence']:.1%})")

        print("\n‚úÖ TEST COMPLETE: Clinical metrics calculated successfully")
        print()

    def run_all_tests(self):
        """Run complete validation suite"""
        if not self.load_model_and_data():
            print("‚ùå Failed to load model and data. Exiting.")
            return

        try:
            self.test_reproducibility()
            self.test_variance_reduction()
            self.test_prediction_stability()
            self.test_performance_benchmarking()
            self.test_clinical_decision_support()

            print("=" * 70)
            print("üéâ ALL TESTS COMPLETE")
            print("=" * 70)
            print("\nüìù Summary:")
            print("   ‚úÖ Ensemble averaging implementation validated")
            print("   ‚úÖ Variance reduction demonstrated")
            print("   ‚úÖ Prediction stability confirmed")
            print("   ‚úÖ Performance within acceptable limits")
            print("   ‚úÖ Clinical decision support metrics working")
            print("\nüí° Next steps:")
            print("   1. Integrate ensemble inference into demo/app.py")
            print("   2. Update Flask API to use ensemble_size parameter")
            print("   3. Run full test set evaluation (Phase 2.1)")
            print()

        except Exception as e:
            print(f"\n‚ùå Test suite failed with error: {e}")
            import traceback
            traceback.print_exc()


def main():
    """Main entry point"""
    import argparse

    parser = argparse.ArgumentParser(description="Validate ensemble averaging implementation")
    parser.add_argument(
        '--model-path',
        type=str,
        default='models/best_model.pt',
        help='Path to trained model checkpoint'
    )

    args = parser.parse_args()

    validator = EnsembleValidator(model_path=args.model_path)
    validator.run_all_tests()


if __name__ == "__main__":
    main()
